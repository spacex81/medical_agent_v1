{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8b7ce4-0b82-4d62-bcbc-e38ff87d68c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"=== DUR PRODUCT INFO - ALL 9 APIs EXTRACTION (SPARK CONNECT COMPATIBLE) ===\")\n",
    "\n",
    "# Configuration\n",
    "ENCODED_SERVICE_KEY = \"h9Dbf2cz0HOrqZb5BIqrfrti%2FD5zZLTYAxFpQuywAB7ZUx3yb67jBDuD5uNlHvAszz9c14NffOmMNQjGv5FzwA%3D%3D\"\n",
    "SERVICE_KEY = urllib.parse.unquote(ENCODED_SERVICE_KEY)\n",
    "BASE_URL = \"https://apis.data.go.kr/1471000/DURPrdlstInfoService03\"\n",
    "\n",
    "# All 9 API configurations (ordered by size: small ‚Üí large)\n",
    "API_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"Ìà¨Ïó¨Í∏∞Í∞ÑÏ£ºÏùò\",\n",
    "        \"api_id\": 6,\n",
    "        \"endpoint\": \"getMdctnPdAtentInfoList03\",\n",
    "        \"expected_records\": 642,\n",
    "        \"table_name\": \"main.default.dur_product_duration_bronze\",\n",
    "        \"description\": \"Administration Period Precautions\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ÎÖ∏Ïù∏Ï£ºÏùò\",\n",
    "        \"api_id\": 2,\n",
    "        \"endpoint\": \"getOdsnAtentInfoList03\",\n",
    "        \"expected_records\": 2052,\n",
    "        \"table_name\": \"main.default.dur_product_elderly_bronze\",\n",
    "        \"description\": \"Elderly Precautions\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ÏÑúÎ∞©Ï†ïÎ∂ÑÌï†Ï£ºÏùò\",\n",
    "        \"api_id\": 8,\n",
    "        \"endpoint\": \"getSeobangjeongPartitnAtentInfoList03\",\n",
    "        \"expected_records\": 2157,\n",
    "        \"table_name\": \"main.default.dur_product_extended_bronze\",\n",
    "        \"description\": \"Extended Release Division Precautions\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ÌäπÏ†ïÏó∞Î†πÎåÄÍ∏àÍ∏∞\",\n",
    "        \"api_id\": 4,\n",
    "        \"endpoint\": \"getSpcifyAgrdeTabooInfoList03\",\n",
    "        \"expected_records\": 2692,\n",
    "        \"table_name\": \"main.default.dur_product_age_bronze\",\n",
    "        \"description\": \"Specific Age Group Contraindications\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ïö©ÎüâÏ£ºÏùò\",\n",
    "        \"api_id\": 5,\n",
    "        \"endpoint\": \"getCpctyAtentInfoList03\",\n",
    "        \"expected_records\": 6782,\n",
    "        \"table_name\": \"main.default.dur_product_dosage_bronze\",\n",
    "        \"description\": \"Dosage Precautions\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ìö®Îä•Íµ∞Ï§ëÎ≥µ\",\n",
    "        \"api_id\": 7,\n",
    "        \"endpoint\": \"getEfcyDplctInfoList03\",\n",
    "        \"expected_records\": 7118,\n",
    "        \"table_name\": \"main.default.dur_product_efficacy_bronze\",\n",
    "        \"description\": \"Efficacy Group Duplication\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ÏûÑÎ∂ÄÍ∏àÍ∏∞\",\n",
    "        \"api_id\": 9,\n",
    "        \"endpoint\": \"getPwnmTabooInfoList03\",\n",
    "        \"expected_records\": 16353,\n",
    "        \"table_name\": \"main.default.dur_product_pregnancy_bronze\",\n",
    "        \"description\": \"Pregnancy Contraindications\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DURÌíàÎ™©Ï†ïÎ≥¥\",\n",
    "        \"api_id\": 3,\n",
    "        \"endpoint\": \"getDurPrdlstInfoList03\",\n",
    "        \"expected_records\": 24065,\n",
    "        \"table_name\": \"main.default.dur_product_info_bronze\",\n",
    "        \"description\": \"General DUR Product Info\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Î≥ëÏö©Í∏àÍ∏∞\",\n",
    "        \"api_id\": 1,\n",
    "        \"endpoint\": \"getUsjntTabooInfoList03\",\n",
    "        \"expected_records\": 240873,\n",
    "        \"table_name\": \"main.default.dur_product_interaction_bronze\",\n",
    "        \"description\": \"Drug Interaction Contraindications\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def clean_record(record):\n",
    "    \"\"\"Clean record - convert all values to strings\"\"\"\n",
    "    cleaned = {}\n",
    "    for key, value in record.items():\n",
    "        cleaned[key] = \"\" if value is None else str(value)\n",
    "    return cleaned\n",
    "\n",
    "def make_api_call(endpoint, page_no, num_rows=100):\n",
    "    \"\"\"Make API call\"\"\"\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    params = {\n",
    "        \"serviceKey\": SERVICE_KEY,\n",
    "        \"pageNo\": page_no,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"type\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    header = data.get(\"header\", {})\n",
    "    if header.get(\"resultCode\") != \"00\":\n",
    "        raise Exception(f\"API Error: {header.get('resultMsg')}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_data(config):\n",
    "    \"\"\"Extract data from API\"\"\"\n",
    "    print(f\"\\nüöÄ Extracting {config['name']} data...\")\n",
    "    print(f\"   Expected: {config['expected_records']:,} records\")\n",
    "    \n",
    "    all_records = []\n",
    "    page_no = 1\n",
    "    total_count = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            if page_no > 1:\n",
    "                # Dynamic delay based on dataset size\n",
    "                delay = 0.3 if config['expected_records'] < 5000 else 0.5\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            data = make_api_call(config['endpoint'], page_no)\n",
    "            \n",
    "            body = data.get(\"body\", {})\n",
    "            if total_count is None:\n",
    "                total_count = body.get(\"totalCount\", 0)\n",
    "                estimated_pages = (total_count + 99) // 100\n",
    "                print(f\"   üìä Total records: {total_count:,} ({estimated_pages:,} pages)\")\n",
    "            \n",
    "            items = body.get(\"items\", [])\n",
    "            if not items:\n",
    "                break\n",
    "            \n",
    "            # Clean each record\n",
    "            cleaned_items = [clean_record(item) for item in items]\n",
    "            all_records.extend(cleaned_items)\n",
    "            \n",
    "            # Progress reporting based on dataset size\n",
    "            if config['expected_records'] > 50000:\n",
    "                # Large dataset: report every 100 pages\n",
    "                if page_no % 100 == 0 or len(all_records) >= total_count:\n",
    "                    progress = len(all_records) / total_count * 100 if total_count > 0 else 0\n",
    "                    print(f\"   ‚úÖ Page {page_no}: {len(all_records):,}/{total_count:,} ({progress:.1f}%)\")\n",
    "            elif config['expected_records'] > 10000:\n",
    "                # Medium dataset: report every 50 pages\n",
    "                if page_no % 50 == 0 or len(all_records) >= total_count:\n",
    "                    progress = len(all_records) / total_count * 100 if total_count > 0 else 0\n",
    "                    print(f\"   ‚úÖ Page {page_no}: {len(all_records):,}/{total_count:,} ({progress:.1f}%)\")\n",
    "            else:\n",
    "                # Small dataset: report every 10 pages\n",
    "                if page_no % 10 == 0 or len(all_records) >= total_count:\n",
    "                    print(f\"   ‚úÖ Page {page_no}: +{len(items)} | Total: {len(all_records):,}\")\n",
    "            \n",
    "            if len(all_records) >= total_count:\n",
    "                break\n",
    "            \n",
    "            page_no += 1\n",
    "            \n",
    "            # Safety check\n",
    "            max_expected_pages = (config['expected_records'] + 99) // 100 + 20\n",
    "            if page_no > max_expected_pages:\n",
    "                print(f\"   ‚ö†Ô∏è  Safety break at page {page_no}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error on page {page_no}: {str(e)}\")\n",
    "            if page_no == 1:\n",
    "                break\n",
    "            page_no += 1\n",
    "            continue\n",
    "    \n",
    "    completeness = len(all_records) / config['expected_records'] * 100\n",
    "    print(f\"   üéâ Extraction complete: {len(all_records):,} records ({completeness:.1f}%)\")\n",
    "    return all_records, total_count\n",
    "\n",
    "def create_bronze_table(config, records):\n",
    "    \"\"\"Create bronze table using Spark Connect compatible method\"\"\"\n",
    "    if not records:\n",
    "        print(f\"   ‚ùå No records to process for {config['name']}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üíæ Creating bronze table for {config['name']}...\")\n",
    "        \n",
    "        # Primary method: Direct DataFrame creation (Spark Connect compatible)\n",
    "        df = spark.createDataFrame(records)\n",
    "        \n",
    "        print(f\"   ‚úÖ DataFrame created: {df.count():,} records, {len(df.columns)} columns\")\n",
    "        \n",
    "        # Write to Delta table\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(config['table_name'])\n",
    "        \n",
    "        print(f\"   ‚úÖ Bronze table created: {config['table_name']}\")\n",
    "        \n",
    "        # Verification\n",
    "        verification_df = spark.table(config['table_name'])\n",
    "        verified_count = verification_df.count()\n",
    "        \n",
    "        print(f\"   üìä Verification: {verified_count:,} records in table\")\n",
    "        \n",
    "        return {\n",
    "            'name': config['name'],\n",
    "            'table_name': config['table_name'],\n",
    "            'records': verified_count,\n",
    "            'expected': config['expected_records'],\n",
    "            'columns': len(df.columns)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error creating table for {config['name']}: {str(e)}\")\n",
    "        print(f\"   üîÑ Trying SQL approach...\")\n",
    "        \n",
    "        try:\n",
    "            # Fallback: SQL approach\n",
    "            temp_df = spark.createDataFrame(records)\n",
    "            temp_view_name = f\"temp_{config['name'].replace(' ', '_')}\"\n",
    "            temp_df.createOrReplaceTempView(temp_view_name)\n",
    "            \n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE OR REPLACE TABLE {config['table_name']}\n",
    "                USING DELTA\n",
    "                AS SELECT * FROM {temp_view_name}\n",
    "            \"\"\")\n",
    "            \n",
    "            print(f\"   ‚úÖ Bronze table created via SQL: {config['table_name']}\")\n",
    "            \n",
    "            verification_df = spark.table(config['table_name'])\n",
    "            verified_count = verification_df.count()\n",
    "            print(f\"   üìä Verification: {verified_count:,} records in table\")\n",
    "            \n",
    "            return {\n",
    "                'name': config['name'],\n",
    "                'table_name': config['table_name'],\n",
    "                'records': verified_count,\n",
    "                'expected': config['expected_records'],\n",
    "                'columns': len(temp_df.columns)\n",
    "            }\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå SQL approach also failed for {config['name']}: {str(e2)}\")\n",
    "            return None\n",
    "\n",
    "def process_api(config):\n",
    "    \"\"\"Process a single API end-to-end\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing API {config['api_id']}/9: {config['name']}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract data\n",
    "        records, total_count = extract_data(config)\n",
    "        \n",
    "        if not records:\n",
    "            print(f\"   ‚ùå No data extracted for {config['name']}\")\n",
    "            return None\n",
    "        \n",
    "        # Create bronze table\n",
    "        result = create_bronze_table(config, records)\n",
    "        \n",
    "        if result:\n",
    "            processing_time = time.time() - start_time\n",
    "            result['processing_time'] = processing_time\n",
    "            \n",
    "            records_per_sec = len(records) / processing_time if processing_time > 0 else 0\n",
    "            print(f\"   ‚è±Ô∏è  Completed in {processing_time/60:.1f} minutes ({records_per_sec:.0f} rec/sec)\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   üí• {config['name']} failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution - process all 9 APIs sequentially\"\"\"\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    total_expected = sum(config['expected_records'] for config in API_CONFIGS)\n",
    "    \n",
    "    print(f\"üìã Processing ALL {len(API_CONFIGS)} DUR Product APIs:\")\n",
    "    print(f\"   Total expected records: {total_expected:,}\")\n",
    "    print(f\"   Estimated processing time: {total_expected//5000} minutes\")\n",
    "    \n",
    "    for i, config in enumerate(API_CONFIGS, 1):\n",
    "        size = \"üî• LARGE\" if config['expected_records'] > 50000 else \"üìä MEDIUM\" if config['expected_records'] > 5000 else \"‚ö° SMALL\"\n",
    "        print(f\"   {i}. {config['name']} ({size}: {config['expected_records']:,} records)\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Starting sequential extraction (most stable approach)...\")\n",
    "    \n",
    "    results = []\n",
    "    failed_apis = []\n",
    "    \n",
    "    for i, config in enumerate(API_CONFIGS, 1):\n",
    "        result = process_api(config)\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "        else:\n",
    "            failed_apis.append(config['name'])\n",
    "        \n",
    "        # Brief pause between APIs (except after the last one)\n",
    "        if i < len(API_CONFIGS):\n",
    "            print(f\"\\n   ‚è∏Ô∏è  Brief pause before next API...\")\n",
    "            time.sleep(3)\n",
    "    \n",
    "    # Final comprehensive summary\n",
    "    overall_time = time.time() - overall_start\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéâ === ALL DUR PRODUCT APIs PROCESSING COMPLETED ===\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"‚è±Ô∏è  Total processing time: {overall_time/60:.1f} minutes\")\n",
    "    print(f\"‚úÖ Successful extractions: {len(results)}/{len(API_CONFIGS)}\")\n",
    "    \n",
    "    if failed_apis:\n",
    "        print(f\"‚ùå Failed APIs: {', '.join(failed_apis)}\")\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nüìä Detailed Summary Report:\")\n",
    "        print(f\"{'API Name':<20} {'Records':<12} {'Expected':<12} {'Complete':<10} {'Columns':<8} {'Time':<8}\")\n",
    "        print(f\"{'-'*85}\")\n",
    "        \n",
    "        total_records = 0\n",
    "        total_expected_final = 0\n",
    "        \n",
    "        for result in results:\n",
    "            total_records += result['records']\n",
    "            total_expected_final += result['expected']\n",
    "            completeness = result['records'] / result['expected'] * 100\n",
    "            time_min = result['processing_time'] / 60\n",
    "            \n",
    "            print(f\"{result['name']:<20} \"\n",
    "                  f\"{result['records']:<12,} \"\n",
    "                  f\"{result['expected']:<12,} \"\n",
    "                  f\"{completeness:<9.1f}% \"\n",
    "                  f\"{result['columns']:<8} \"\n",
    "                  f\"{time_min:<7.1f}m\")\n",
    "        \n",
    "        print(f\"{'-'*85}\")\n",
    "        overall_completeness = total_records / total_expected_final * 100 if total_expected_final > 0 else 0\n",
    "        print(f\"{'TOTAL':<20} {total_records:<12,} {total_expected_final:<12,} {overall_completeness:<9.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüíæ Bronze Tables Successfully Created:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"   {i}. ‚úÖ {result['table_name']}\")\n",
    "        \n",
    "        print(f\"\\nüèÜ Final Success Summary:\")\n",
    "        print(f\"   üìä Total records extracted: {total_records:,}\")\n",
    "        print(f\"   üéØ Overall completeness: {overall_completeness:.1f}%\")\n",
    "        print(f\"   ‚ö° Average extraction speed: {total_records/(overall_time/60):.0f} records/minute\")\n",
    "        print(f\"   üèóÔ∏è  Bronze tables ready for silver layer transformations!\")\n",
    "        \n",
    "        # Performance insights\n",
    "        if len(results) > 1:\n",
    "            fastest_api = max(results, key=lambda x: x['records']/x['processing_time'] if x['processing_time'] > 0 else 0)\n",
    "            slowest_api = min(results, key=lambda x: x['records']/x['processing_time'] if x['processing_time'] > 0 else float('inf'))\n",
    "            \n",
    "            print(f\"\\n‚ö° Performance Insights:\")\n",
    "            print(f\"   Fastest: {fastest_api['name']} ({fastest_api['records']/fastest_api['processing_time']:.0f} rec/sec)\")\n",
    "            print(f\"   Slowest: {slowest_api['name']} ({slowest_api['records']/slowest_api['processing_time']:.0f} rec/sec)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå No APIs were processed successfully\")\n",
    "        print(f\"üí° Check API connectivity and service key\")\n",
    "\n",
    "# Execute the comprehensive extraction\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dur_product_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
