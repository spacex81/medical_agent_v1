{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ccd808-6d44-419e-aeda-1e1ed952ae56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"=== DUR PRODUCT APIs 1 & 3 - FULL EXTRACTION (ALL RECORDS) ===\")\n",
    "\n",
    "# Configuration\n",
    "ENCODED_SERVICE_KEY = \"h9Dbf2cz0HOrqZb5BIqrfrti%2FD5zZLTYAxFpQuywAB7ZUx3yb67jBDuD5uNlHvAszz9c14NffOmMNQjGv5FzwA%3D%3D\"\n",
    "SERVICE_KEY = urllib.parse.unquote(ENCODED_SERVICE_KEY)\n",
    "BASE_URL = \"https://apis.data.go.kr/1471000/DURPrdlstInfoService03\"\n",
    "\n",
    "# Full extraction configuration\n",
    "API_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"Î≥ëÏö©Í∏àÍ∏∞\",\n",
    "        \"api_id\": 1,\n",
    "        \"endpoint\": \"getUsjntTabooInfoList03\",\n",
    "        \"expected_records\": 240873,\n",
    "        \"table_name\": \"main.default.dur_product_interaction_bronze\",\n",
    "        \"description\": \"Drug Interaction Contraindications\",\n",
    "        \"size_category\": \"LARGE\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DURÌíàÎ™©Ï†ïÎ≥¥\",\n",
    "        \"api_id\": 3,\n",
    "        \"endpoint\": \"getDurPrdlstInfoList03\",\n",
    "        \"expected_records\": 24065,\n",
    "        \"table_name\": \"main.default.dur_product_info_bronze\",\n",
    "        \"description\": \"General DUR Product Info\",\n",
    "        \"size_category\": \"MEDIUM\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Thread lock for safe logging\n",
    "log_lock = threading.Lock()\n",
    "\n",
    "def safe_print(message):\n",
    "    \"\"\"Thread-safe printing with timestamp\"\"\"\n",
    "    with log_lock:\n",
    "        timestamp = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Clean column names to remove invalid characters\"\"\"\n",
    "    old_columns = df.columns\n",
    "    new_columns = []\n",
    "    \n",
    "    for col_name in old_columns:\n",
    "        # Remove invalid characters, spaces, and normalize\n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', col_name.strip())\n",
    "        clean_name = re.sub(r'_+', '_', clean_name).strip('_')\n",
    "        \n",
    "        if clean_name and clean_name[0].isdigit():\n",
    "            clean_name = 'col_' + clean_name\n",
    "        if not clean_name:\n",
    "            clean_name = f'col_{len(new_columns)}'\n",
    "            \n",
    "        new_columns.append(clean_name)\n",
    "    \n",
    "    # Rename columns\n",
    "    for old_col, new_col in zip(old_columns, new_columns):\n",
    "        if old_col != new_col:\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    return df, dict(zip(old_columns, new_columns))\n",
    "\n",
    "def clean_record(record):\n",
    "    \"\"\"Clean record - convert all values to strings\"\"\"\n",
    "    cleaned = {}\n",
    "    for key, value in record.items():\n",
    "        cleaned[key] = \"\" if value is None else str(value)\n",
    "    return cleaned\n",
    "\n",
    "def make_api_call(endpoint, page_no, num_rows=100):\n",
    "    \"\"\"Make API call with retry logic\"\"\"\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    params = {\n",
    "        \"serviceKey\": SERVICE_KEY,\n",
    "        \"pageNo\": page_no,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"type\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            header = data.get(\"header\", {})\n",
    "            if header.get(\"resultCode\") != \"00\":\n",
    "                raise Exception(f\"API Error: {header.get('resultMsg')}\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (attempt + 1) * 2  # Exponential backoff\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "def extract_full_data(config):\n",
    "    \"\"\"Extract all data from API with enhanced progress tracking\"\"\"\n",
    "    safe_print(f\"üöÄ Starting {config['name']} FULL extraction...\")\n",
    "    safe_print(f\"   Expected: {config['expected_records']:,} records ({config['size_category']})\")\n",
    "    \n",
    "    all_records = []\n",
    "    page_no = 1\n",
    "    total_count = None\n",
    "    last_progress_report = 0\n",
    "    errors_count = 0\n",
    "    max_errors = 10\n",
    "    \n",
    "    # Progress reporting intervals based on size\n",
    "    if config['size_category'] == 'LARGE':\n",
    "        progress_interval = 5.0  # Report every 5%\n",
    "        page_report_interval = 100  # Report every 100 pages\n",
    "    else:\n",
    "        progress_interval = 10.0  # Report every 10%\n",
    "        page_report_interval = 50   # Report every 50 pages\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Dynamic delay based on dataset size and page number\n",
    "            if page_no > 1:\n",
    "                if config['size_category'] == 'LARGE':\n",
    "                    delay = 0.5 if page_no % 100 == 0 else 0.3  # Longer delay every 100 pages\n",
    "                else:\n",
    "                    delay = 0.3\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            data = make_api_call(config['endpoint'], page_no)\n",
    "            \n",
    "            body = data.get(\"body\", {})\n",
    "            if total_count is None:\n",
    "                total_count = body.get(\"totalCount\", 0)\n",
    "                estimated_pages = (total_count + 99) // 100\n",
    "                estimated_time = estimated_pages * 0.4 / 60  # Rough estimate in minutes\n",
    "                safe_print(f\"   üìä {config['name']}: {total_count:,} records ({estimated_pages:,} pages)\")\n",
    "                safe_print(f\"   ‚è±Ô∏è  Estimated processing time: {estimated_time:.0f} minutes\")\n",
    "            \n",
    "            items = body.get(\"items\", [])\n",
    "            if not items:\n",
    "                safe_print(f\"   ‚úÖ {config['name']}: No more data at page {page_no}\")\n",
    "                break\n",
    "            \n",
    "            # Clean each record\n",
    "            cleaned_items = [clean_record(item) for item in items]\n",
    "            all_records.extend(cleaned_items)\n",
    "            \n",
    "            # Progress reporting\n",
    "            current_progress = (len(all_records) / total_count * 100) if total_count > 0 else 0\n",
    "            \n",
    "            # Report based on progress percentage or page intervals\n",
    "            should_report = (\n",
    "                (current_progress - last_progress_report >= progress_interval) or\n",
    "                (page_no % page_report_interval == 0) or\n",
    "                (len(all_records) >= total_count)\n",
    "            )\n",
    "            \n",
    "            if should_report:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                records_per_min = (len(all_records) / elapsed_time * 60) if elapsed_time > 0 else 0\n",
    "                remaining_records = total_count - len(all_records)\n",
    "                eta_minutes = (remaining_records / records_per_min) if records_per_min > 0 else 0\n",
    "                \n",
    "                safe_print(f\"   ‚úÖ {config['name']} Page {page_no}: {len(all_records):,}/{total_count:,} \"\n",
    "                          f\"({current_progress:.1f}%) | {records_per_min:.0f} rec/min | ETA: {eta_minutes:.0f}min\")\n",
    "                last_progress_report = current_progress\n",
    "            \n",
    "            # Check completion\n",
    "            if len(all_records) >= total_count:\n",
    "                safe_print(f\"   üéâ {config['name']}: All records collected!\")\n",
    "                break\n",
    "            \n",
    "            page_no += 1\n",
    "            \n",
    "            # Safety check\n",
    "            max_expected_pages = (config['expected_records'] + 99) // 100 + 50\n",
    "            if page_no > max_expected_pages:\n",
    "                safe_print(f\"   ‚ö†Ô∏è  {config['name']}: Safety break at page {page_no}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors_count += 1\n",
    "            safe_print(f\"   ‚ùå {config['name']}: Error on page {page_no} (error {errors_count}/{max_errors}): {str(e)}\")\n",
    "            \n",
    "            if errors_count >= max_errors:\n",
    "                safe_print(f\"   üõë {config['name']}: Too many errors, stopping extraction\")\n",
    "                break\n",
    "            \n",
    "            if page_no == 1:\n",
    "                safe_print(f\"   üõë {config['name']}: Critical error on first page, stopping\")\n",
    "                break\n",
    "            \n",
    "            # Continue with next page for transient errors\n",
    "            page_no += 1\n",
    "            time.sleep(2)  # Wait longer after error\n",
    "            continue\n",
    "    \n",
    "    # Final extraction summary\n",
    "    total_time = time.time() - start_time\n",
    "    completeness = (len(all_records) / config['expected_records'] * 100) if config['expected_records'] > 0 else 0\n",
    "    \n",
    "    safe_print(f\"   üèÅ {config['name']}: Extraction complete!\")\n",
    "    safe_print(f\"      Records: {len(all_records):,} ({completeness:.1f}% complete)\")\n",
    "    safe_print(f\"      Time: {total_time/60:.1f} minutes\")\n",
    "    safe_print(f\"      Performance: {len(all_records)/(total_time/60):.0f} records/minute\")\n",
    "    safe_print(f\"      Errors: {errors_count}\")\n",
    "    \n",
    "    return all_records, total_count\n",
    "\n",
    "def create_bronze_table(config, records):\n",
    "    \"\"\"Create bronze table with enhanced error handling\"\"\"\n",
    "    if not records:\n",
    "        safe_print(f\"   ‚ùå {config['name']}: No records to process\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        safe_print(f\"   üíæ {config['name']}: Creating bronze table...\")\n",
    "        safe_print(f\"      Processing {len(records):,} records...\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df_start = time.time()\n",
    "        df = spark.createDataFrame(records)\n",
    "        df_time = time.time() - df_start\n",
    "        \n",
    "        # Clean column names\n",
    "        clean_start = time.time()\n",
    "        cleaned_df, column_mapping = clean_column_names(df)\n",
    "        clean_time = time.time() - clean_start\n",
    "        \n",
    "        if column_mapping:\n",
    "            cleaned_count = sum(1 for old, new in column_mapping.items() if old != new)\n",
    "            safe_print(f\"      üîß Cleaned {cleaned_count} column names in {clean_time:.1f}s\")\n",
    "        \n",
    "        safe_print(f\"      ‚úÖ DataFrame ready: {cleaned_df.count():,} records, {len(cleaned_df.columns)} columns ({df_time:.1f}s)\")\n",
    "        \n",
    "        # Write to Delta table\n",
    "        write_start = time.time()\n",
    "        cleaned_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(config['table_name'])\n",
    "        write_time = time.time() - write_start\n",
    "        \n",
    "        safe_print(f\"      ‚úÖ Bronze table created: {config['table_name']} ({write_time:.1f}s)\")\n",
    "        \n",
    "        # Verification\n",
    "        verification_df = spark.table(config['table_name'])\n",
    "        verified_count = verification_df.count()\n",
    "        \n",
    "        safe_print(f\"      üìä Verification: {verified_count:,} records in table\")\n",
    "        \n",
    "        # Data integrity check\n",
    "        integrity_ok = verified_count == len(records)\n",
    "        safe_print(f\"      üîç Data integrity: {'‚úÖ Perfect' if integrity_ok else '‚ö†Ô∏è Check needed'}\")\n",
    "        \n",
    "        return {\n",
    "            'name': config['name'],\n",
    "            'table_name': config['table_name'],\n",
    "            'records': verified_count,\n",
    "            'expected': config['expected_records'],\n",
    "            'columns': len(cleaned_df.columns),\n",
    "            'column_mapping': column_mapping,\n",
    "            'integrity_ok': integrity_ok\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        safe_print(f\"   ‚ùå {config['name']}: Error creating table - {str(e)}\")\n",
    "        safe_print(f\"   üîÑ Trying SQL fallback approach...\")\n",
    "        \n",
    "        try:\n",
    "            # SQL fallback\n",
    "            temp_df = spark.createDataFrame(records)\n",
    "            cleaned_temp_df, _ = clean_column_names(temp_df)\n",
    "            \n",
    "            temp_view_name = f\"temp_api_{config['api_id']}_full\"\n",
    "            cleaned_temp_df.createOrReplaceTempView(temp_view_name)\n",
    "            \n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE OR REPLACE TABLE {config['table_name']}\n",
    "                USING DELTA\n",
    "                AS SELECT * FROM {temp_view_name}\n",
    "            \"\"\")\n",
    "            \n",
    "            safe_print(f\"      ‚úÖ Bronze table created via SQL: {config['table_name']}\")\n",
    "            \n",
    "            verification_df = spark.table(config['table_name'])\n",
    "            verified_count = verification_df.count()\n",
    "            \n",
    "            return {\n",
    "                'name': config['name'],\n",
    "                'table_name': config['table_name'],\n",
    "                'records': verified_count,\n",
    "                'expected': config['expected_records'],\n",
    "                'columns': len(cleaned_temp_df.columns),\n",
    "                'column_mapping': {},\n",
    "                'integrity_ok': verified_count == len(records)\n",
    "            }\n",
    "            \n",
    "        except Exception as e2:\n",
    "            safe_print(f\"   ‚ùå {config['name']}: SQL fallback also failed - {str(e2)}\")\n",
    "            return None\n",
    "\n",
    "def process_full_api(config):\n",
    "    \"\"\"Process a single API full extraction\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    safe_print(f\"\\n{'='*60}\")\n",
    "    safe_print(f\"Processing API {config['api_id']}: {config['name']} (FULL)\")\n",
    "    safe_print(f\"Description: {config['description']}\")\n",
    "    safe_print(f\"Size: {config['size_category']} - {config['expected_records']:,} records\")\n",
    "    safe_print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract all data\n",
    "        records, total_count = extract_full_data(config)\n",
    "        \n",
    "        if not records:\n",
    "            safe_print(f\"   ‚ùå {config['name']}: No data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Create bronze table\n",
    "        result = create_bronze_table(config, records)\n",
    "        \n",
    "        if result:\n",
    "            processing_time = time.time() - start_time\n",
    "            result['processing_time'] = processing_time\n",
    "            \n",
    "            records_per_sec = len(records) / processing_time if processing_time > 0 else 0\n",
    "            safe_print(f\"   ‚è±Ô∏è  {config['name']}: COMPLETED in {processing_time/60:.1f} minutes ({records_per_sec:.0f} rec/sec)\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        safe_print(f\"   üí• {config['name']}: FAILED - {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution - process both APIs in parallel\"\"\"\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    total_expected = sum(config['expected_records'] for config in API_CONFIGS)\n",
    "    \n",
    "    print(f\"üìã Processing APIs 1 & 3 - Full Extraction:\")\n",
    "    print(f\"   Total expected records: {total_expected:,}\")\n",
    "    print(f\"   Estimated total time: 60-90 minutes\")\n",
    "    \n",
    "    for config in API_CONFIGS:\n",
    "        print(f\"   API {config['api_id']}: {config['name']} ({config['size_category']}: {config['expected_records']:,} records)\")\n",
    "        print(f\"      Table: {config['table_name']}\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Starting parallel full extraction...\")\n",
    "    print(f\"   ‚ö†Ô∏è  Large dataset - this will take significant time\")\n",
    "    print(f\"   üìä Progress will be reported regularly\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process both APIs in parallel\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = [executor.submit(process_full_api, config) for config in API_CONFIGS]\n",
    "        \n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "    \n",
    "    # Final comprehensive summary\n",
    "    overall_time = time.time() - overall_start\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéâ === FULL EXTRACTION COMPLETED ===\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"‚è±Ô∏è  Total processing time: {overall_time/60:.1f} minutes\")\n",
    "    print(f\"‚úÖ Successful extractions: {len(results)}/{len(API_CONFIGS)}\")\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nüìä Final Results Summary:\")\n",
    "        print(f\"{'API Name':<15} {'Records':<12} {'Expected':<12} {'Complete':<10} {'Columns':<8} {'Time':<10} {'Integrity':<10}\")\n",
    "        print(f\"{'-'*85}\")\n",
    "        \n",
    "        total_records = 0\n",
    "        total_expected_final = 0\n",
    "        \n",
    "        for result in results:\n",
    "            total_records += result['records']\n",
    "            total_expected_final += result['expected']\n",
    "            completeness = result['records'] / result['expected'] * 100\n",
    "            time_min = result['processing_time'] / 60\n",
    "            integrity = \"‚úÖ OK\" if result['integrity_ok'] else \"‚ö†Ô∏è Check\"\n",
    "            \n",
    "            print(f\"{result['name']:<15} \"\n",
    "                  f\"{result['records']:<12,} \"\n",
    "                  f\"{result['expected']:<12,} \"\n",
    "                  f\"{completeness:<9.1f}% \"\n",
    "                  f\"{result['columns']:<8} \"\n",
    "                  f\"{time_min:<9.1f}m \"\n",
    "                  f\"{integrity:<10}\")\n",
    "        \n",
    "        print(f\"{'-'*85}\")\n",
    "        overall_completeness = total_records / total_expected_final * 100 if total_expected_final > 0 else 0\n",
    "        print(f\"{'TOTAL':<15} {total_records:<12,} {total_expected_final:<12,} {overall_completeness:<9.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüíæ Bronze Tables Created:\")\n",
    "        for result in results:\n",
    "            print(f\"   ‚úÖ {result['table_name']}\")\n",
    "        \n",
    "        print(f\"\\nüèÜ Final Success Summary:\")\n",
    "        print(f\"   üìä Total records extracted: {total_records:,}\")\n",
    "        print(f\"   üéØ Overall completeness: {overall_completeness:.1f}%\")\n",
    "        print(f\"   ‚ö° Average extraction speed: {total_records/(overall_time/60):.0f} records/minute\")\n",
    "        print(f\"   üèóÔ∏è  Bronze tables ready for silver layer transformations!\")\n",
    "        \n",
    "        # Performance insights\n",
    "        if len(results) >= 2:\n",
    "            api1_result = next((r for r in results if 'interaction' in r['table_name']), None)\n",
    "            api3_result = next((r for r in results if 'info' in r['table_name']), None)\n",
    "            \n",
    "            if api1_result and api3_result:\n",
    "                api1_speed = api1_result['records'] / (api1_result['processing_time'] / 60)\n",
    "                api3_speed = api3_result['records'] / (api3_result['processing_time'] / 60)\n",
    "                \n",
    "                print(f\"\\n‚ö° Performance Comparison:\")\n",
    "                print(f\"   API 1 (Large): {api1_speed:.0f} records/minute\")\n",
    "                print(f\"   API 3 (Medium): {api3_speed:.0f} records/minute\")\n",
    "        \n",
    "        # Column cleaning summary\n",
    "        total_columns_cleaned = sum(len([v for k, v in r.get('column_mapping', {}).items() if k != v]) for r in results)\n",
    "        if total_columns_cleaned > 0:\n",
    "            print(f\"\\nüîß Schema Cleaning Summary:\")\n",
    "            print(f\"   Total column names cleaned: {total_columns_cleaned}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"\\n‚ùå No extractions completed successfully\")\n",
    "        print(f\"üí° Check logs above for specific error details\")\n",
    "\n",
    "# Execute the full extraction\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dur_product_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
