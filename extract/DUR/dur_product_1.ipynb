{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1429367-b463-46d3-b035-3cf9294e0906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"=== DUR PRODUCT API 1 - FINAL CHUNKED EXTRACTION ===\")\n",
    "\n",
    "# Configuration\n",
    "ENCODED_SERVICE_KEY = \"h9Dbf2cz0HOrqZb5BIqrfrti%2FD5zZLTYAxFpQuywAB7ZUx3yb67jBDuD5uNlHvAszz9c14NffOmMNQjGv5FzwA%3D%3D\"\n",
    "SERVICE_KEY = urllib.parse.unquote(ENCODED_SERVICE_KEY)\n",
    "BASE_URL = \"https://apis.data.go.kr/1471000/DURPrdlstInfoService03/getUsjntTabooInfoList03\"\n",
    "\n",
    "# API 1 Configuration\n",
    "API_CONFIG = {\n",
    "    \"name\": \"Î≥ëÏö©Í∏àÍ∏∞\",\n",
    "    \"api_id\": 1,\n",
    "    \"endpoint\": \"getUsjntTabooInfoList03\",\n",
    "    \"expected_records\": 240873,\n",
    "    \"table_name\": \"main.default.dur_product_interaction_bronze\",\n",
    "    \"description\": \"Drug Interaction Contraindications\"\n",
    "}\n",
    "\n",
    "# Chunking configuration\n",
    "CHUNK_SIZE = 5000  # Process 5000 records at a time (50 API pages)\n",
    "PAGES_PER_CHUNK = 50  # 50 pages * 100 records = 5000 records per chunk\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Clean column names to remove invalid characters\"\"\"\n",
    "    old_columns = df.columns\n",
    "    new_columns = []\n",
    "    \n",
    "    for col_name in old_columns:\n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', col_name.strip())\n",
    "        clean_name = re.sub(r'_+', '_', clean_name).strip('_')\n",
    "        \n",
    "        if clean_name and clean_name[0].isdigit():\n",
    "            clean_name = 'col_' + clean_name\n",
    "        if not clean_name:\n",
    "            clean_name = f'col_{len(new_columns)}'\n",
    "            \n",
    "        new_columns.append(clean_name)\n",
    "    \n",
    "    for old_col, new_col in zip(old_columns, new_columns):\n",
    "        if old_col != new_col:\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_record(record):\n",
    "    \"\"\"Clean record - convert all values to strings\"\"\"\n",
    "    cleaned = {}\n",
    "    for key, value in record.items():\n",
    "        cleaned[key] = \"\" if value is None else str(value)\n",
    "    return cleaned\n",
    "\n",
    "def make_api_call(page_no, num_rows=100):\n",
    "    \"\"\"Make API call with retry logic - using working format from debug\"\"\"\n",
    "    params = {\n",
    "        \"serviceKey\": SERVICE_KEY,  # Using the working decoded version\n",
    "        \"pageNo\": page_no,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"type\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, */*'\n",
    "    }\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(BASE_URL, params=params, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse JSON\n",
    "            data = response.json()\n",
    "            \n",
    "            # Check API response status\n",
    "            header = data.get(\"header\", {})\n",
    "            if header.get(\"resultCode\") != \"00\":\n",
    "                raise Exception(f\"API Error: {header.get('resultMsg')}\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (attempt + 1) * 2\n",
    "                print(f\"      ‚ö†Ô∏è  Retry {attempt + 1}/3 for page {page_no}: {str(e)}\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "        except json.JSONDecodeError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (attempt + 1) * 2\n",
    "                print(f\"      ‚ö†Ô∏è  JSON retry {attempt + 1}/3 for page {page_no}: {str(e)}\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "def extract_chunk(start_page, end_page, chunk_num, total_chunks):\n",
    "    \"\"\"Extract a chunk of data (pages start_page to end_page)\"\"\"\n",
    "    print(f\"   üì• Extracting chunk {chunk_num}/{total_chunks}: pages {start_page}-{end_page}...\")\n",
    "    \n",
    "    chunk_records = []\n",
    "    errors_in_chunk = 0\n",
    "    max_errors_per_chunk = 5\n",
    "    \n",
    "    chunk_start_time = time.time()\n",
    "    \n",
    "    for page_no in range(start_page, end_page + 1):\n",
    "        try:\n",
    "            if page_no > start_page:\n",
    "                time.sleep(0.3)  # Respectful delay\n",
    "            \n",
    "            data = make_api_call(page_no)\n",
    "            \n",
    "            body = data.get(\"body\", {})\n",
    "            items = body.get(\"items\", [])\n",
    "            \n",
    "            if not items:\n",
    "                print(f\"      ‚ö†Ô∏è  No items on page {page_no}, chunk may be complete\")\n",
    "                break\n",
    "            \n",
    "            # Clean each record\n",
    "            cleaned_items = [clean_record(item) for item in items]\n",
    "            chunk_records.extend(cleaned_items)\n",
    "            \n",
    "            # Progress within chunk (report every 10 pages or at end)\n",
    "            if page_no % 10 == 0 or page_no == end_page:\n",
    "                elapsed = time.time() - chunk_start_time\n",
    "                pages_done = page_no - start_page + 1\n",
    "                pages_remaining = end_page - page_no\n",
    "                eta_chunk = (elapsed / pages_done * pages_remaining) if pages_done > 0 else 0\n",
    "                \n",
    "                print(f\"      ‚úÖ Page {page_no}: +{len(items)} | Chunk: {len(chunk_records)} | ETA: {eta_chunk:.0f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors_in_chunk += 1\n",
    "            print(f\"      ‚ùå Error on page {page_no}: {str(e)}\")\n",
    "            \n",
    "            if errors_in_chunk >= max_errors_per_chunk:\n",
    "                print(f\"      üõë Too many errors in chunk {chunk_num}, stopping chunk\")\n",
    "                break\n",
    "            \n",
    "            continue  # Skip this page and continue\n",
    "    \n",
    "    chunk_time = time.time() - chunk_start_time\n",
    "    print(f\"   ‚úÖ Chunk {chunk_num} extracted: {len(chunk_records)} records in {chunk_time:.1f}s (errors: {errors_in_chunk})\")\n",
    "    return chunk_records, errors_in_chunk\n",
    "\n",
    "def write_chunk_to_table(chunk_records, chunk_number, is_first_chunk, total_records_so_far):\n",
    "    \"\"\"Write a chunk of records to the Delta table\"\"\"\n",
    "    if not chunk_records:\n",
    "        print(f\"   ‚ö†Ô∏è  Chunk {chunk_number}: No records to write\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üíæ Writing chunk {chunk_number}: {len(chunk_records)} records...\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df_start = time.time()\n",
    "        df = spark.createDataFrame(chunk_records)\n",
    "        \n",
    "        # Clean column names (important for API 1)\n",
    "        cleaned_df = clean_column_names(df)\n",
    "        df_time = time.time() - df_start\n",
    "        \n",
    "        print(f\"      üìä DataFrame ready: {cleaned_df.count()} records, {len(cleaned_df.columns)} columns ({df_time:.1f}s)\")\n",
    "        \n",
    "        # Write to Delta table\n",
    "        write_start = time.time()\n",
    "        \n",
    "        if is_first_chunk:\n",
    "            # First chunk: overwrite to create/reset table\n",
    "            print(f\"      üîÑ Creating new table (overwrite mode)\")\n",
    "            cleaned_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(API_CONFIG['table_name'])\n",
    "            print(f\"      ‚úÖ New table created: {API_CONFIG['table_name']}\")\n",
    "        else:\n",
    "            # Subsequent chunks: append\n",
    "            print(f\"      ‚ûï Appending to existing table\")\n",
    "            cleaned_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .saveAsTable(API_CONFIG['table_name'])\n",
    "        \n",
    "        write_time = time.time() - write_start\n",
    "        print(f\"      ‚úÖ Chunk {chunk_number} written successfully ({write_time:.1f}s)\")\n",
    "        \n",
    "        # Verification\n",
    "        current_count = spark.table(API_CONFIG['table_name']).count()\n",
    "        expected_count = total_records_so_far + len(chunk_records)\n",
    "        \n",
    "        print(f\"      üìä Table verification: {current_count:,} records (expected: {expected_count:,})\")\n",
    "        \n",
    "        if current_count != expected_count:\n",
    "            print(f\"      ‚ö†Ô∏è  Count mismatch - expected {expected_count:,}, got {current_count:,}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Error writing chunk {chunk_number}: {str(e)}\")\n",
    "        \n",
    "        # Try SQL approach as fallback\n",
    "        try:\n",
    "            print(f\"      üîÑ Trying SQL fallback for chunk {chunk_number}...\")\n",
    "            \n",
    "            temp_df = spark.createDataFrame(chunk_records)\n",
    "            cleaned_temp_df = clean_column_names(temp_df)\n",
    "            \n",
    "            temp_view_name = f\"temp_chunk_{chunk_number}\"\n",
    "            cleaned_temp_df.createOrReplaceTempView(temp_view_name)\n",
    "            \n",
    "            if is_first_chunk:\n",
    "                spark.sql(f\"\"\"\n",
    "                    CREATE OR REPLACE TABLE {API_CONFIG['table_name']}\n",
    "                    USING DELTA\n",
    "                    AS SELECT * FROM {temp_view_name}\n",
    "                \"\"\")\n",
    "            else:\n",
    "                spark.sql(f\"\"\"\n",
    "                    INSERT INTO {API_CONFIG['table_name']}\n",
    "                    SELECT * FROM {temp_view_name}\n",
    "                \"\"\")\n",
    "            \n",
    "            print(f\"      ‚úÖ Chunk {chunk_number} written via SQL fallback\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"      ‚ùå SQL fallback also failed for chunk {chunk_number}: {str(e2)}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main chunked extraction process\"\"\"\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    print(f\"üéØ Target: {API_CONFIG['name']} ({API_CONFIG['description']})\")\n",
    "    print(f\"   Expected records: {API_CONFIG['expected_records']:,}\")\n",
    "    print(f\"   Table: {API_CONFIG['table_name']}\")\n",
    "    print(f\"   Strategy: Chunked processing ({CHUNK_SIZE:,} records per chunk)\")\n",
    "    \n",
    "    # Get actual total count\n",
    "    print(f\"\\nüìä Verifying API connectivity and total count...\")\n",
    "    try:\n",
    "        data = make_api_call(1, 1)  # Test call\n",
    "        body = data.get(\"body\", {})\n",
    "        total_count = body.get(\"totalCount\", 0)\n",
    "        \n",
    "        print(f\"   ‚úÖ API connectivity confirmed\")\n",
    "        print(f\"   üìä API reports: {total_count:,} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå API connectivity test failed: {str(e)}\")\n",
    "        print(f\"   üîÑ Using expected count as fallback\")\n",
    "        total_count = API_CONFIG['expected_records']\n",
    "    \n",
    "    # Calculate chunks\n",
    "    total_pages = (total_count + 99) // 100\n",
    "    total_chunks = (total_pages + PAGES_PER_CHUNK - 1) // PAGES_PER_CHUNK\n",
    "    estimated_time_min = total_chunks * 2  # ~2 minutes per chunk\n",
    "    \n",
    "    print(f\"   üìÑ Total pages: {total_pages:,}\")\n",
    "    print(f\"   üì¶ Total chunks: {total_chunks}\")\n",
    "    print(f\"   ‚è±Ô∏è  Estimated time: {estimated_time_min} minutes\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting chunked extraction...\")\n",
    "    print(f\"   üí° Each chunk processes {PAGES_PER_CHUNK} pages (~{CHUNK_SIZE:,} records)\")\n",
    "    print(f\"   üß† Memory efficient: Only ~5MB per chunk vs 500MB+ for full load\")\n",
    "    \n",
    "    # Process chunks\n",
    "    successful_chunks = 0\n",
    "    failed_chunks = 0\n",
    "    total_records_written = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for chunk_num in range(1, total_chunks + 1):\n",
    "        chunk_overall_start = time.time()\n",
    "        \n",
    "        # Calculate page range for this chunk\n",
    "        start_page = (chunk_num - 1) * PAGES_PER_CHUNK + 1\n",
    "        end_page = min(chunk_num * PAGES_PER_CHUNK, total_pages)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Processing Chunk {chunk_num}/{total_chunks}\")\n",
    "        print(f\"Pages {start_page}-{end_page} ({end_page - start_page + 1} pages)\")\n",
    "        overall_progress = (chunk_num - 1) / total_chunks * 100\n",
    "        print(f\"Overall Progress: {overall_progress:.1f}% | Records so far: {total_records_written:,}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract chunk\n",
    "            chunk_records, chunk_errors = extract_chunk(start_page, end_page, chunk_num, total_chunks)\n",
    "            total_errors += chunk_errors\n",
    "            \n",
    "            if chunk_records:\n",
    "                # Write chunk\n",
    "                write_success = write_chunk_to_table(\n",
    "                    chunk_records, \n",
    "                    chunk_num, \n",
    "                    is_first_chunk=(chunk_num == 1),\n",
    "                    total_records_so_far=total_records_written\n",
    "                )\n",
    "                \n",
    "                if write_success:\n",
    "                    successful_chunks += 1\n",
    "                    total_records_written += len(chunk_records)\n",
    "                    \n",
    "                    # Progress update\n",
    "                    chunk_overall_time = time.time() - chunk_overall_start\n",
    "                    overall_elapsed = time.time() - overall_start\n",
    "                    overall_progress_updated = chunk_num / total_chunks * 100\n",
    "                    \n",
    "                    # ETA calculation\n",
    "                    chunks_remaining = total_chunks - chunk_num\n",
    "                    avg_time_per_chunk = overall_elapsed / chunk_num\n",
    "                    eta_minutes = (chunks_remaining * avg_time_per_chunk) / 60\n",
    "                    \n",
    "                    print(f\"   üéâ Chunk {chunk_num} COMPLETED in {chunk_overall_time:.1f}s\")\n",
    "                    print(f\"   üìä Progress: {overall_progress_updated:.1f}% | Total records: {total_records_written:,}\")\n",
    "                    print(f\"   ‚è±Ô∏è  ETA: {eta_minutes:.0f} minutes remaining\")\n",
    "                    \n",
    "                else:\n",
    "                    failed_chunks += 1\n",
    "                    print(f\"   ‚ùå Chunk {chunk_num} extraction succeeded but write failed\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Chunk {chunk_num}: No data extracted\")\n",
    "                failed_chunks += 1\n",
    "            \n",
    "            # Brief pause between chunks\n",
    "            if chunk_num < total_chunks:\n",
    "                print(f\"   ‚è∏Ô∏è  Brief pause before next chunk...\")\n",
    "                time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   üí• Chunk {chunk_num} failed completely: {str(e)}\")\n",
    "            failed_chunks += 1\n",
    "            continue\n",
    "    \n",
    "    # Final comprehensive summary\n",
    "    overall_time = time.time() - overall_start\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üéâ === CHUNKED EXTRACTION COMPLETED ===\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Final verification\n",
    "    try:\n",
    "        final_table_count = spark.table(API_CONFIG['table_name']).count()\n",
    "        completeness = (final_table_count / total_count * 100) if total_count > 0 else 0\n",
    "        \n",
    "        print(f\"üìä Final Results:\")\n",
    "        print(f\"   ‚úÖ Successful chunks: {successful_chunks}/{total_chunks}\")\n",
    "        print(f\"   ‚ùå Failed chunks: {failed_chunks}\")\n",
    "        print(f\"   üìä Records written: {total_records_written:,}\")\n",
    "        print(f\"   üìä Final table count: {final_table_count:,}\")\n",
    "        print(f\"   üéØ Completeness: {completeness:.1f}%\")\n",
    "        print(f\"   ‚ö†Ô∏è  Total API errors: {total_errors}\")\n",
    "        print(f\"   ‚è±Ô∏è  Total processing time: {overall_time/60:.1f} minutes\")\n",
    "        print(f\"   ‚ö° Average speed: {final_table_count/(overall_time/60):.0f} records/minute\")\n",
    "        \n",
    "        print(f\"\\nüíæ Bronze Table: {API_CONFIG['table_name']}\")\n",
    "        \n",
    "        if final_table_count > 0:\n",
    "            print(f\"   ‚úÖ Table created successfully with {final_table_count:,} records!\")\n",
    "            print(f\"   üìà Ready for silver layer transformations\")\n",
    "            \n",
    "            # Show schema info\n",
    "            table_df = spark.table(API_CONFIG['table_name'])\n",
    "            print(f\"   üìã Schema: {len(table_df.columns)} columns\")\n",
    "            \n",
    "            # Quick sample\n",
    "            print(f\"\\nüìù Sample Data (first 3 records):\")\n",
    "            table_df.show(3, truncate=True)\n",
    "            \n",
    "            # Success assessment\n",
    "            if completeness >= 95:\n",
    "                print(f\"\\nüèÜ EXTRACTION HIGHLY SUCCESSFUL!\")\n",
    "                print(f\"   ‚úÖ {completeness:.1f}% completeness achieved\")\n",
    "                print(f\"   ‚úÖ Memory-efficient chunked approach worked perfectly\")\n",
    "                print(f\"   ‚úÖ Ready for analytics and silver layer processing\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  EXTRACTION PARTIALLY SUCCESSFUL\")\n",
    "                print(f\"   üìä {completeness:.1f}% completeness - some data missing\")\n",
    "                print(f\"   üí° Consider re-running failed chunks if needed\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"   ‚ùå No data in final table - extraction failed\")\n",
    "        \n",
    "        # Memory efficiency summary\n",
    "        print(f\"\\nüß† Memory Efficiency Achieved:\")\n",
    "        print(f\"   üì¶ Processed in {total_chunks} chunks of ~{CHUNK_SIZE:,} records each\")\n",
    "        print(f\"   üíæ Peak memory: ~5MB per chunk (vs ~500MB for full load)\")\n",
    "        print(f\"   üîÑ Sustainable approach for large datasets\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in final verification: {str(e)}\")\n",
    "\n",
    "# Execute the chunked extraction\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dur_product_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
