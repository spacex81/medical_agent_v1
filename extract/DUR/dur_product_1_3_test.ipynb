{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12509893-ed99-4c11-bd02-b35b556e1ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"=== DUR PRODUCT APIs 1 & 3 - TEST EXTRACTION (300 records each, parallel) ===\")\n",
    "\n",
    "# Configuration\n",
    "ENCODED_SERVICE_KEY = \"h9Dbf2cz0HOrqZb5BIqrfrti%2FD5zZLTYAxFpQuywAB7ZUx3yb67jBDuD5uNlHvAszz9c14NffOmMNQjGv5FzwA%3D%3D\"\n",
    "SERVICE_KEY = urllib.parse.unquote(ENCODED_SERVICE_KEY)\n",
    "BASE_URL = \"https://apis.data.go.kr/1471000/DURPrdlstInfoService03\"\n",
    "\n",
    "# Test configuration - Only APIs 1 and 3 with 300 records each\n",
    "API_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"Î≥ëÏö©Í∏àÍ∏∞\",\n",
    "        \"api_id\": 1,\n",
    "        \"endpoint\": \"getUsjntTabooInfoList03\",\n",
    "        \"expected_records\": 240873,\n",
    "        \"test_records\": 300,  # Only extract 300 for testing\n",
    "        \"table_name\": \"main.default.dur_product_interaction_bronze_test\",\n",
    "        \"description\": \"Drug Interaction Contraindications\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DURÌíàÎ™©Ï†ïÎ≥¥\",\n",
    "        \"api_id\": 3,\n",
    "        \"endpoint\": \"getDurPrdlstInfoList03\",\n",
    "        \"expected_records\": 24065,\n",
    "        \"test_records\": 300,  # Only extract 300 for testing\n",
    "        \"table_name\": \"main.default.dur_product_info_bronze_test\",\n",
    "        \"description\": \"General DUR Product Info\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Thread lock for safe logging\n",
    "log_lock = threading.Lock()\n",
    "\n",
    "def safe_print(message):\n",
    "    \"\"\"Thread-safe printing\"\"\"\n",
    "    with log_lock:\n",
    "        print(message)\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Clean column names to remove invalid characters\"\"\"\n",
    "    # Get current column names\n",
    "    old_columns = df.columns\n",
    "    new_columns = []\n",
    "    \n",
    "    for col_name in old_columns:\n",
    "        # Remove invalid characters and spaces\n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', col_name)\n",
    "        # Remove leading/trailing underscores and multiple underscores\n",
    "        clean_name = re.sub(r'_+', '_', clean_name).strip('_')\n",
    "        # Ensure it doesn't start with a number\n",
    "        if clean_name and clean_name[0].isdigit():\n",
    "            clean_name = 'col_' + clean_name\n",
    "        # Handle empty names\n",
    "        if not clean_name:\n",
    "            clean_name = f'col_{len(new_columns)}'\n",
    "        new_columns.append(clean_name)\n",
    "    \n",
    "    # Rename columns\n",
    "    for old_col, new_col in zip(old_columns, new_columns):\n",
    "        if old_col != new_col:\n",
    "            df = df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    return df, dict(zip(old_columns, new_columns))\n",
    "\n",
    "def clean_record(record):\n",
    "    \"\"\"Clean record - convert all values to strings\"\"\"\n",
    "    cleaned = {}\n",
    "    for key, value in record.items():\n",
    "        cleaned[key] = \"\" if value is None else str(value)\n",
    "    return cleaned\n",
    "\n",
    "def make_api_call(endpoint, page_no, num_rows=100):\n",
    "    \"\"\"Make API call\"\"\"\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    params = {\n",
    "        \"serviceKey\": SERVICE_KEY,\n",
    "        \"pageNo\": page_no,\n",
    "        \"numOfRows\": num_rows,\n",
    "        \"type\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    header = data.get(\"header\", {})\n",
    "    if header.get(\"resultCode\") != \"00\":\n",
    "        raise Exception(f\"API Error: {header.get('resultMsg')}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_test_data(config):\n",
    "    \"\"\"Extract limited test data from API\"\"\"\n",
    "    safe_print(f\"\\nüöÄ Extracting {config['name']} TEST data...\")\n",
    "    safe_print(f\"   Target: {config['test_records']} records (from {config['expected_records']:,} total)\")\n",
    "    \n",
    "    all_records = []\n",
    "    page_no = 1\n",
    "    target_records = config['test_records']\n",
    "    \n",
    "    while len(all_records) < target_records:\n",
    "        try:\n",
    "            if page_no > 1:\n",
    "                time.sleep(0.3)\n",
    "            \n",
    "            # Calculate how many records to request this page\n",
    "            remaining = target_records - len(all_records)\n",
    "            records_to_request = min(100, remaining)\n",
    "            \n",
    "            data = make_api_call(config['endpoint'], page_no, records_to_request)\n",
    "            \n",
    "            body = data.get(\"body\", {})\n",
    "            if page_no == 1:\n",
    "                total_available = body.get(\"totalCount\", 0)\n",
    "                safe_print(f\"   üìä {config['name']}: {total_available:,} records available (extracting {target_records})\")\n",
    "            \n",
    "            items = body.get(\"items\", [])\n",
    "            if not items:\n",
    "                safe_print(f\"   ‚ö†Ô∏è  {config['name']}: No more items at page {page_no}\")\n",
    "                break\n",
    "            \n",
    "            # Clean each record\n",
    "            cleaned_items = [clean_record(item) for item in items]\n",
    "            all_records.extend(cleaned_items)\n",
    "            \n",
    "            safe_print(f\"   ‚úÖ {config['name']} Page {page_no}: +{len(items)} | Total: {len(all_records)}\")\n",
    "            \n",
    "            # Stop when we have enough records\n",
    "            if len(all_records) >= target_records:\n",
    "                all_records = all_records[:target_records]  # Trim to exact target\n",
    "                break\n",
    "            \n",
    "            page_no += 1\n",
    "            \n",
    "            # Safety check\n",
    "            if page_no > 10:  # Should only need 3 pages for 300 records\n",
    "                safe_print(f\"   ‚ö†Ô∏è  {config['name']}: Safety break at page {page_no}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            safe_print(f\"   ‚ùå {config['name']}: Error on page {page_no}: {str(e)}\")\n",
    "            if page_no == 1:\n",
    "                break\n",
    "            page_no += 1\n",
    "            continue\n",
    "    \n",
    "    safe_print(f\"   üéâ {config['name']}: Test extraction complete - {len(all_records)} records\")\n",
    "    return all_records\n",
    "\n",
    "def create_test_bronze_table(config, records):\n",
    "    \"\"\"Create test bronze table with column name cleaning\"\"\"\n",
    "    if not records:\n",
    "        safe_print(f\"   ‚ùå {config['name']}: No records to process\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        safe_print(f\"   üíæ {config['name']}: Creating test bronze table...\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = spark.createDataFrame(records)\n",
    "        \n",
    "        # Clean column names to handle invalid characters\n",
    "        cleaned_df, column_mapping = clean_column_names(df)\n",
    "        \n",
    "        if column_mapping:\n",
    "            safe_print(f\"   üîß {config['name']}: Cleaned {len(column_mapping)} column names\")\n",
    "            # Show a few examples of renamed columns\n",
    "            examples = list(column_mapping.items())[:3]\n",
    "            for old, new in examples:\n",
    "                if old != new:\n",
    "                    safe_print(f\"      '{old}' ‚Üí '{new}'\")\n",
    "        \n",
    "        safe_print(f\"   ‚úÖ {config['name']}: DataFrame created - {cleaned_df.count()} records, {len(cleaned_df.columns)} columns\")\n",
    "        \n",
    "        # Write to Delta table\n",
    "        cleaned_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(config['table_name'])\n",
    "        \n",
    "        safe_print(f\"   ‚úÖ {config['name']}: Test bronze table created - {config['table_name']}\")\n",
    "        \n",
    "        # Verification\n",
    "        verification_df = spark.table(config['table_name'])\n",
    "        verified_count = verification_df.count()\n",
    "        \n",
    "        safe_print(f\"   üìä {config['name']}: Verification - {verified_count} records in table\")\n",
    "        \n",
    "        return {\n",
    "            'name': config['name'],\n",
    "            'table_name': config['table_name'],\n",
    "            'records': verified_count,\n",
    "            'target': config['test_records'],\n",
    "            'columns': len(cleaned_df.columns),\n",
    "            'column_mapping': column_mapping\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        safe_print(f\"   ‚ùå {config['name']}: Error creating table - {str(e)}\")\n",
    "        \n",
    "        # Try SQL approach with cleaned names\n",
    "        try:\n",
    "            safe_print(f\"   üîÑ {config['name']}: Trying SQL approach...\")\n",
    "            \n",
    "            temp_df = spark.createDataFrame(records)\n",
    "            cleaned_temp_df, _ = clean_column_names(temp_df)\n",
    "            \n",
    "            # Use simple temp view name\n",
    "            temp_view_name = f\"temp_api_{config['api_id']}\"\n",
    "            cleaned_temp_df.createOrReplaceTempView(temp_view_name)\n",
    "            \n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE OR REPLACE TABLE {config['table_name']}\n",
    "                USING DELTA\n",
    "                AS SELECT * FROM {temp_view_name}\n",
    "            \"\"\")\n",
    "            \n",
    "            safe_print(f\"   ‚úÖ {config['name']}: Test table created via SQL - {config['table_name']}\")\n",
    "            \n",
    "            verification_df = spark.table(config['table_name'])\n",
    "            verified_count = verification_df.count()\n",
    "            \n",
    "            return {\n",
    "                'name': config['name'],\n",
    "                'table_name': config['table_name'],\n",
    "                'records': verified_count,\n",
    "                'target': config['test_records'],\n",
    "                'columns': len(cleaned_temp_df.columns),\n",
    "                'column_mapping': {}\n",
    "            }\n",
    "            \n",
    "        except Exception as e2:\n",
    "            safe_print(f\"   ‚ùå {config['name']}: SQL approach also failed - {str(e2)}\")\n",
    "            return None\n",
    "\n",
    "def process_api_test(config):\n",
    "    \"\"\"Process a single API test extraction\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    safe_print(f\"\\n{'='*50}\")\n",
    "    safe_print(f\"Testing API {config['api_id']}: {config['name']}\")\n",
    "    safe_print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract test data\n",
    "        records = extract_test_data(config)\n",
    "        \n",
    "        if not records:\n",
    "            safe_print(f\"   ‚ùå {config['name']}: No test data extracted\")\n",
    "            return None\n",
    "        \n",
    "        # Create test bronze table\n",
    "        result = create_test_bronze_table(config, records)\n",
    "        \n",
    "        if result:\n",
    "            processing_time = time.time() - start_time\n",
    "            result['processing_time'] = processing_time\n",
    "            \n",
    "            records_per_sec = len(records) / processing_time if processing_time > 0 else 0\n",
    "            safe_print(f\"   ‚è±Ô∏è  {config['name']}: Test completed in {processing_time:.1f}s ({records_per_sec:.0f} rec/sec)\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        safe_print(f\"   üí• {config['name']}: Test failed - {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution - parallel test of APIs 1 and 3\"\"\"\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    print(f\"üß™ Testing problematic APIs with limited data:\")\n",
    "    for config in API_CONFIGS:\n",
    "        print(f\"   API {config['api_id']}: {config['name']} (extracting {config['test_records']} from {config['expected_records']:,})\")\n",
    "        print(f\"      Test table: {config['table_name']}\")\n",
    "    \n",
    "    print(f\"\\nüîÑ Starting parallel test extraction...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process both APIs in parallel\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = [executor.submit(process_api_test, config) for config in API_CONFIGS]\n",
    "        \n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "    \n",
    "    # Summary\n",
    "    overall_time = time.time() - overall_start\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéâ === TEST EXTRACTION COMPLETED ===\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚è±Ô∏è  Total processing time: {overall_time:.1f} seconds\")\n",
    "    print(f\"‚úÖ Successful tests: {len(results)}/{len(API_CONFIGS)}\")\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nüìä Test Results:\")\n",
    "        print(f\"{'API Name':<15} {'Records':<8} {'Target':<8} {'Success':<8} {'Columns':<8} {'Time':<8}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        \n",
    "        for result in results:\n",
    "            success_rate = result['records'] / result['target'] * 100\n",
    "            time_sec = result['processing_time']\n",
    "            \n",
    "            print(f\"{result['name']:<15} \"\n",
    "                  f\"{result['records']:<8} \"\n",
    "                  f\"{result['target']:<8} \"\n",
    "                  f\"{success_rate:<7.1f}% \"\n",
    "                  f\"{result['columns']:<8} \"\n",
    "                  f\"{time_sec:<7.1f}s\")\n",
    "        \n",
    "        print(f\"\\nüíæ Test Tables Created:\")\n",
    "        for result in results:\n",
    "            print(f\"   ‚úÖ {result['table_name']}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Test Assessment:\")\n",
    "        all_success = all(r['records'] == r['target'] for r in results)\n",
    "        \n",
    "        if all_success:\n",
    "            print(f\"   ‚úÖ All tests passed perfectly!\")\n",
    "            print(f\"   üöÄ Ready to scale to full extraction\")\n",
    "            print(f\"   üîß Column cleaning approach validated\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Some tests had partial success\")\n",
    "            print(f\"   üîç Review results before full extraction\")\n",
    "        \n",
    "        # Column mapping info\n",
    "        for result in results:\n",
    "            if result.get('column_mapping'):\n",
    "                print(f\"\\n   üìã {result['name']} column mapping applied\")\n",
    "    else:\n",
    "        print(f\"‚ùå No tests completed successfully\")\n",
    "        print(f\"üí° Review API connectivity and schema issues\")\n",
    "\n",
    "# Execute the test\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dur_product_1_3_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
