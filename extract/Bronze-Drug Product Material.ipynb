{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1464e2b-7c57-4c8b-a22b-5b31b5cd077a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 1: FILE STRUCTURE EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84aa8bfe-35ab-4707-a634-3324daf7eb26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Base path for the JSON files\n",
    "base_path = '/Volumes/main/default/medical_data_volume/medical_data_complete_unzipped/medical_data_local/drug-product-material-info/'\n",
    "\n",
    "# Check if directory exists and list files\n",
    "print(f\"\\n1. Checking directory: {base_path}\")\n",
    "\n",
    "try:\n",
    "    # Use dbutils to list files in the directory\n",
    "    files = dbutils.fs.ls(base_path)\n",
    "    \n",
    "    # Filter for JSON files and sort them\n",
    "    json_files = [f for f in files if f.name.endswith('.json') and f.name.startswith('chunk_')]\n",
    "    json_files.sort(key=lambda x: int(x.name.split('_')[1].split('.')[0]))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    # Check if we have all expected files (chunk_001.json to chunk_131.json)\n",
    "    expected_files = [f\"chunk_{i:03d}.json\" for i in range(1, 132)]\n",
    "    actual_files = [f.name for f in json_files]\n",
    "    \n",
    "    missing_files = set(expected_files) - set(actual_files)\n",
    "    extra_files = set(actual_files) - set(expected_files)\n",
    "    \n",
    "    print(f\"\\n2. File Inventory:\")\n",
    "    print(f\"   Expected files: 131 (chunk_001.json to chunk_131.json)\")\n",
    "    print(f\"   Found files: {len(actual_files)}\")\n",
    "    print(f\"   Missing files: {len(missing_files)}\")\n",
    "    print(f\"   Extra files: {len(extra_files)}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"   Missing: {sorted(list(missing_files))[:10]}...\")  # Show first 10\n",
    "    if extra_files:\n",
    "        print(f\"   Extra: {sorted(list(extra_files))[:10]}...\")  # Show first 10\n",
    "    \n",
    "    # Check file sizes (handle potential access issues)\n",
    "    print(f\"\\n3. File Size Analysis:\")\n",
    "    try:\n",
    "        file_sizes = [(f.name, f.size) for f in json_files]\n",
    "        file_sizes.sort(key=lambda x: x[1])  # Sort by size\n",
    "        \n",
    "        total_size = sum(size for _, size in file_sizes)\n",
    "        avg_size = total_size / len(file_sizes) if file_sizes else 0\n",
    "        min_size = min(file_sizes, key=lambda x: x[1]) if file_sizes else None\n",
    "        max_size = max(file_sizes, key=lambda x: x[1]) if file_sizes else None\n",
    "        \n",
    "        print(f\"   Total size: {total_size / (1024*1024):.2f} MB\")\n",
    "        print(f\"   Average size: {avg_size / (1024*1024):.2f} MB\")\n",
    "        print(f\"   Smallest file: {min_size[0]} ({min_size[1] / (1024*1024):.2f} MB)\")\n",
    "        print(f\"   Largest file: {max_size[0]} ({max_size[1] / (1024*1024):.2f} MB)\")\n",
    "        \n",
    "    except Exception as size_error:\n",
    "        print(f\"   Could not access file size information: {str(size_error)}\")\n",
    "        print(f\"   Proceeding with schema analysis...\")\n",
    "    \n",
    "    # Sample a few files to check their structure\n",
    "    print(f\"\\n4. Sample File Structure Analysis:\")\n",
    "    \n",
    "    # Use direct file paths for testing\n",
    "    sample_file_names = ['chunk_001.json', 'chunk_066.json', 'chunk_131.json']\n",
    "    \n",
    "    for i, file_name in enumerate(sample_file_names, 1):\n",
    "        file_path = f\"{base_path}{file_name}\"\n",
    "        print(f\"\\n   Sample {i}: {file_name}\")\n",
    "        print(f\"   Full path: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Try to read the file\n",
    "            sample_df = spark.read.option(\"multiline\", \"true\").option(\"encoding\", \"UTF-8\").json(file_path)\n",
    "            print(f\"   ✅ File readable\")\n",
    "            print(f\"   Records: {sample_df.count()}\")\n",
    "            print(f\"   Columns: {len(sample_df.columns)}\")\n",
    "            print(f\"   Schema preview:\")\n",
    "            \n",
    "            # Show first few columns of schema\n",
    "            schema_info = sample_df.dtypes\n",
    "            for j, (col_name, col_type) in enumerate(schema_info[:10]):\n",
    "                print(f\"     {j+1:2d}. {col_name}: {col_type}\")\n",
    "            \n",
    "            if len(sample_df.columns) > 10:\n",
    "                print(f\"     ... and {len(sample_df.columns) - 10} more columns\")\n",
    "            \n",
    "            # Show a sample record\n",
    "            print(f\"   Sample record:\")\n",
    "            sample_df.show(1, truncate=False, vertical=True)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ ERROR reading file: {str(e)}\")\n",
    "            print(f\"   Error type: {type(e).__name__}\")\n",
    "    \n",
    "    print(f\"\\n=== EXPLORATION COMPLETE ===\")\n",
    "    print(f\"Ready for Step 2: Schema Validation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {str(e)}\")\n",
    "    print(\"Make sure the path is correct and accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e6e931c-debe-45da-b5e6-50dd9d9bed87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 2: SCHEMA VALIDATION ACROSS MULTIPLE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26729ae-93b2-4d66-8f5e-e2fa9db4867b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "base_path = '/Volumes/main/default/medical_data_volume/medical_data_complete_unzipped/medical_data_local/drug-product-material-info/'\n",
    "\n",
    "# Test files to validate schema consistency\n",
    "test_files = [\n",
    "    'chunk_001.json',  # First file\n",
    "    'chunk_010.json',  # Early file\n",
    "    'chunk_050.json',  # Middle file\n",
    "    'chunk_100.json',  # Late file\n",
    "    'chunk_131.json'   # Last file\n",
    "]\n",
    "\n",
    "schemas = {}\n",
    "record_counts = {}\n",
    "successful_reads = []\n",
    "failed_reads = []\n",
    "\n",
    "print(f\"\\n1. Testing {len(test_files)} sample files for schema consistency:\")\n",
    "\n",
    "for file_name in test_files:\n",
    "    file_path = f\"{base_path}{file_name}\"\n",
    "    print(f\"\\n   Testing: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read the file\n",
    "        df = spark.read.option(\"multiline\", \"true\").option(\"encoding\", \"UTF-8\").json(file_path)\n",
    "        \n",
    "        # Get schema and record count\n",
    "        schema = df.schema\n",
    "        count = df.count()\n",
    "        \n",
    "        # Store results\n",
    "        schemas[file_name] = schema\n",
    "        record_counts[file_name] = count\n",
    "        successful_reads.append(file_name)\n",
    "        \n",
    "        print(f\"   ✅ Success - Records: {count}, Columns: {len(schema.fields)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_reads.append((file_name, str(e)))\n",
    "        print(f\"   ❌ Failed - Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\n2. Schema Validation Results:\")\n",
    "print(f\"   Successful reads: {len(successful_reads)}\")\n",
    "print(f\"   Failed reads: {len(failed_reads)}\")\n",
    "\n",
    "if failed_reads:\n",
    "    print(f\"   Failed files:\")\n",
    "    for file_name, error in failed_reads:\n",
    "        print(f\"     {file_name}: {error}\")\n",
    "\n",
    "if successful_reads:\n",
    "    # Compare schemas\n",
    "    print(f\"\\n3. Schema Consistency Analysis:\")\n",
    "    \n",
    "    # Use first successful schema as reference\n",
    "    reference_file = successful_reads[0]\n",
    "    reference_schema = schemas[reference_file]\n",
    "    reference_fields = {field.name: field.dataType for field in reference_schema.fields}\n",
    "    \n",
    "    print(f\"   Reference schema from: {reference_file}\")\n",
    "    print(f\"   Reference columns: {len(reference_fields)}\")\n",
    "    \n",
    "    # Check if all schemas match\n",
    "    all_schemas_match = True\n",
    "    schema_differences = {}\n",
    "    \n",
    "    for file_name in successful_reads[1:]:\n",
    "        current_schema = schemas[file_name]\n",
    "        current_fields = {field.name: field.dataType for field in current_schema.fields}\n",
    "        \n",
    "        # Compare column names\n",
    "        ref_columns = set(reference_fields.keys())\n",
    "        curr_columns = set(current_fields.keys())\n",
    "        \n",
    "        missing_in_current = ref_columns - curr_columns\n",
    "        extra_in_current = curr_columns - ref_columns\n",
    "        \n",
    "        # Compare data types for common columns\n",
    "        type_differences = []\n",
    "        common_columns = ref_columns & curr_columns\n",
    "        for col in common_columns:\n",
    "            if reference_fields[col] != current_fields[col]:\n",
    "                type_differences.append((col, reference_fields[col], current_fields[col]))\n",
    "        \n",
    "        if missing_in_current or extra_in_current or type_differences:\n",
    "            all_schemas_match = False\n",
    "            schema_differences[file_name] = {\n",
    "                'missing_columns': missing_in_current,\n",
    "                'extra_columns': extra_in_current,\n",
    "                'type_differences': type_differences\n",
    "            }\n",
    "        \n",
    "        print(f\"   {file_name}: Columns={len(current_fields)} \", end=\"\")\n",
    "        if missing_in_current or extra_in_current or type_differences:\n",
    "            print(\"❌ Schema differs\")\n",
    "        else:\n",
    "            print(\"✅ Schema matches\")\n",
    "    \n",
    "    print(f\"\\n4. Detailed Schema Information:\")\n",
    "    print(f\"   All schemas identical: {'✅ Yes' if all_schemas_match else '❌ No'}\")\n",
    "    \n",
    "    if schema_differences:\n",
    "        print(f\"   Schema differences found in {len(schema_differences)} files:\")\n",
    "        for file_name, diffs in schema_differences.items():\n",
    "            print(f\"\\n     {file_name}:\")\n",
    "            if diffs['missing_columns']:\n",
    "                print(f\"       Missing columns: {list(diffs['missing_columns'])}\")\n",
    "            if diffs['extra_columns']:\n",
    "                print(f\"       Extra columns: {list(diffs['extra_columns'])}\")\n",
    "            if diffs['type_differences']:\n",
    "                print(f\"       Type differences:\")\n",
    "                for col, ref_type, curr_type in diffs['type_differences']:\n",
    "                    print(f\"         {col}: {ref_type} -> {curr_type}\")\n",
    "    \n",
    "    # Show complete schema from reference file\n",
    "    print(f\"\\n5. Complete Schema from {reference_file}:\")\n",
    "    reference_df = spark.read.option(\"multiline\", \"true\").option(\"encoding\", \"UTF-8\").json(f\"{base_path}{reference_file}\")\n",
    "    reference_df.printSchema()\n",
    "    \n",
    "    print(f\"\\n6. Record Count Analysis:\")\n",
    "    total_estimated_records = 0\n",
    "    for file_name in successful_reads:\n",
    "        count = record_counts[file_name]\n",
    "        print(f\"   {file_name}: {count:,} records\")\n",
    "        total_estimated_records += count\n",
    "    \n",
    "    avg_records = total_estimated_records / len(successful_reads)\n",
    "    estimated_total = avg_records * 131  # All 131 files\n",
    "    \n",
    "    print(f\"\\n   Average records per file: {avg_records:,.0f}\")\n",
    "    print(f\"   Estimated total records (131 files): {estimated_total:,.0f}\")\n",
    "    \n",
    "    print(f\"\\n7. Sample Data from {reference_file}:\")\n",
    "    reference_df.show(3, truncate=False)\n",
    "\n",
    "print(f\"\\n=== SCHEMA VALIDATION COMPLETE ===\")\n",
    "\n",
    "if all_schemas_match and len(successful_reads) >= 3:\n",
    "    print(f\"✅ RECOMMENDATION: Use wildcard pattern approach - schemas are consistent\")\n",
    "    print(f\"   Next step: Implement wildcard ingestion\")\n",
    "elif len(successful_reads) >= 3:\n",
    "    print(f\"⚠️  RECOMMENDATION: Use union approach with schema handling\")\n",
    "    print(f\"   Next step: Implement union-based ingestion with schema normalization\")\n",
    "else:\n",
    "    print(f\"❌ RECOMMENDATION: Investigate file access issues before proceeding\")\n",
    "    print(f\"   Next step: Debug file access problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87119b26-c3fc-4470-97a6-65cc828aef5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 3: WILDCARD PATTERN INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90aca52b-bf78-4b31-93fc-0a1f10c30236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"Drug Product Material Info - Bronze Table Creation\")\n",
    "\n",
    "# Configuration\n",
    "base_path = '/Volumes/main/default/medical_data_volume/medical_data_complete_unzipped/medical_data_local/drug-product-material-info/'\n",
    "wildcard_path = f\"{base_path}chunk_*.json\"\n",
    "bronze_table_name = \"main.default.drug_product_material_bronze\"\n",
    "\n",
    "print(f\"\\nSource path: {wildcard_path}\")\n",
    "print(f\"Target table: {bronze_table_name}\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"\\n1. Reading all JSON files using wildcard pattern...\")\n",
    "try:\n",
    "    # Read all files at once using wildcard pattern\n",
    "    df = spark.read \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .option(\"encoding\", \"UTF-8\") \\\n",
    "        .json(wildcard_path)\n",
    "    \n",
    "    print(f\"✅ Successfully read all files\")\n",
    "    \n",
    "    # Quick validation\n",
    "    total_records = df.count()\n",
    "    total_columns = len(df.columns)\n",
    "    \n",
    "    print(f\"   Total records: {total_records:,}\")\n",
    "    print(f\"   Total columns: {total_columns}\")\n",
    "    \n",
    "    print(f\"\\n2. Data Quality Checks:\")\n",
    "    \n",
    "    # Check for null values in key columns\n",
    "    key_columns = [\"ITEM_SEQ\", \"PRDUCT\", \"MTRAL_NM\", \"ENTRPS\"]\n",
    "    for col_name in key_columns:\n",
    "        null_count = df.filter(col(col_name).isNull() | (col(col_name) == \"\")).count()\n",
    "        print(f\"   {col_name}: {null_count:,} null/empty values\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    distinct_records = df.distinct().count()\n",
    "    duplicate_count = total_records - distinct_records\n",
    "    print(f\"   Duplicate records: {duplicate_count:,}\")\n",
    "    \n",
    "    # Show schema\n",
    "    print(f\"\\n3. Final Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(f\"\\n4. Creating Bronze Table...\")\n",
    "    \n",
    "    # Write to Delta table\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(bronze_table_name)\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    print(f\"✅ Successfully created bronze table\")\n",
    "    print(f\"   Processing time: {processing_time:.2f} seconds\")\n",
    "    print(f\"   Records per second: {total_records/processing_time:,.0f}\")\n",
    "    \n",
    "    print(f\"\\n5. Bronze Table Verification:\")\n",
    "    \n",
    "    # Verify the table was created\n",
    "    bronze_df = spark.table(bronze_table_name)\n",
    "    verified_count = bronze_df.count()\n",
    "    \n",
    "    print(f\"   Records in bronze table: {verified_count:,}\")\n",
    "    print(f\"   Data integrity: {'✅ Passed' if verified_count == total_records else '❌ Failed'}\")\n",
    "    \n",
    "    print(f\"\\n6. Sample Data from Bronze Table:\")\n",
    "    bronze_df.select(\n",
    "        \"ENTRPS\", \n",
    "        \"PRDUCT\", \n",
    "        \"MTRAL_NM\", \n",
    "        \"MAIN_INGR_ENG\", \n",
    "        \"QNT\",\n",
    "        \"INGD_UNIT_CD\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "    print(f\"\\n7. Data Distribution Analysis:\")\n",
    "    \n",
    "    # Top enterprises by product count\n",
    "    print(f\"   Top 10 Enterprises by Product Count:\")\n",
    "    bronze_df.groupBy(\"ENTRPS\") \\\n",
    "        .agg(countDistinct(\"ITEM_SEQ\").alias(\"unique_products\"),\n",
    "             count(\"*\").alias(\"total_materials\")) \\\n",
    "        .orderBy(col(\"total_materials\").desc()) \\\n",
    "        .show(10, truncate=False)\n",
    "    \n",
    "    # Material distribution\n",
    "    print(f\"   Top 10 Materials by Usage:\")\n",
    "    bronze_df.groupBy(\"MTRAL_NM\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .show(10, truncate=False)\n",
    "    \n",
    "    print(f\"\\n=== INGESTION COMPLETED SUCCESSFULLY ===\")\n",
    "    print(f\"🎉 Bronze table '{bronze_table_name}' created with {verified_count:,} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during ingestion: {str(e)}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Try to provide helpful debugging info\n",
    "    print(f\"\\nDebugging information:\")\n",
    "    print(f\"- Check if all files are accessible\")\n",
    "    print(f\"- Verify Spark cluster has sufficient memory\")\n",
    "    print(f\"- Consider using batch processing approach if memory issues persist\")\n",
    "    \n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze-Drug Product Material",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
