{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e63130-058e-42bf-a662-911888f26245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "base_path = '/Volumes/main/default/medical_data_volume/medical_data_complete_unzipped/medical_data_local/drug-product-permission-info/'\n",
    "\n",
    "print(\"=== DRUG PRODUCT PERMISSION INFO - EXPLORATION ===\")\n",
    "\n",
    "# Check directory and files\n",
    "print(f\"\\n1. Checking directory: {base_path}\")\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(base_path)\n",
    "    json_files = [f for f in files if f.name.endswith('.json') and f.name.startswith('chunk_')]\n",
    "    json_files.sort(key=lambda x: int(x.name.split('_')[1].split('.')[0]))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    # Check expected files (chunk_001.json to chunk_045.json)\n",
    "    expected_files = [f\"chunk_{i:03d}.json\" for i in range(1, 46)]\n",
    "    actual_files = [f.name for f in json_files]\n",
    "    \n",
    "    missing_files = set(expected_files) - set(actual_files)\n",
    "    extra_files = set(actual_files) - set(expected_files)\n",
    "    \n",
    "    print(f\"\\n2. File Inventory:\")\n",
    "    print(f\"   Expected files: 45 (chunk_001.json to chunk_045.json)\")\n",
    "    print(f\"   Found files: {len(actual_files)}\")\n",
    "    print(f\"   Missing files: {len(missing_files)}\")\n",
    "    print(f\"   Extra files: {len(extra_files)}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"   Missing: {sorted(list(missing_files))}\")\n",
    "    if extra_files:\n",
    "        print(f\"   Extra: {sorted(list(extra_files))}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR accessing directory: {str(e)}\")\n",
    "\n",
    "# Test sample files for schema consistency\n",
    "test_files = [\n",
    "    'chunk_001.json',  # First file\n",
    "    'chunk_010.json',  # Early file\n",
    "    'chunk_025.json',  # Middle file\n",
    "    'chunk_040.json',  # Late file\n",
    "    'chunk_045.json'   # Last file\n",
    "]\n",
    "\n",
    "schemas = {}\n",
    "record_counts = {}\n",
    "successful_reads = []\n",
    "failed_reads = []\n",
    "\n",
    "print(f\"\\n3. Testing {len(test_files)} sample files for schema consistency:\")\n",
    "\n",
    "for file_name in test_files:\n",
    "    file_path = f\"{base_path}{file_name}\"\n",
    "    print(f\"\\n   Testing: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        df = spark.read.option(\"multiline\", \"true\").option(\"encoding\", \"UTF-8\").json(file_path)\n",
    "        schema = df.schema\n",
    "        count = df.count()\n",
    "        \n",
    "        schemas[file_name] = schema\n",
    "        record_counts[file_name] = count\n",
    "        successful_reads.append(file_name)\n",
    "        \n",
    "        print(f\"   ✅ Success - Records: {count:,}, Columns: {len(schema.fields)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_reads.append((file_name, str(e)))\n",
    "        print(f\"   ❌ Failed - Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\n4. Schema Validation Results:\")\n",
    "print(f\"   Successful reads: {len(successful_reads)}\")\n",
    "print(f\"   Failed reads: {len(failed_reads)}\")\n",
    "\n",
    "if successful_reads:\n",
    "    # Compare schemas\n",
    "    reference_file = successful_reads[0]\n",
    "    reference_schema = schemas[reference_file]\n",
    "    reference_fields = {field.name: field.dataType for field in reference_schema.fields}\n",
    "    \n",
    "    print(f\"\\n5. Schema Consistency Analysis:\")\n",
    "    print(f\"   Reference schema from: {reference_file}\")\n",
    "    print(f\"   Reference columns: {len(reference_fields)}\")\n",
    "    \n",
    "    all_schemas_match = True\n",
    "    \n",
    "    for file_name in successful_reads[1:]:\n",
    "        current_schema = schemas[file_name]\n",
    "        current_fields = {field.name: field.dataType for field in current_schema.fields}\n",
    "        \n",
    "        ref_columns = set(reference_fields.keys())\n",
    "        curr_columns = set(current_fields.keys())\n",
    "        \n",
    "        missing_in_current = ref_columns - curr_columns\n",
    "        extra_in_current = curr_columns - ref_columns\n",
    "        \n",
    "        type_differences = []\n",
    "        common_columns = ref_columns & curr_columns\n",
    "        for col in common_columns:\n",
    "            if reference_fields[col] != current_fields[col]:\n",
    "                type_differences.append((col, reference_fields[col], current_fields[col]))\n",
    "        \n",
    "        if missing_in_current or extra_in_current or type_differences:\n",
    "            all_schemas_match = False\n",
    "        \n",
    "        print(f\"   {file_name}: Columns={len(current_fields)} \", end=\"\")\n",
    "        if missing_in_current or extra_in_current or type_differences:\n",
    "            print(\"❌ Schema differs\")\n",
    "        else:\n",
    "            print(\"✅ Schema matches\")\n",
    "    \n",
    "    print(f\"\\n6. Complete Schema from {reference_file}:\")\n",
    "    reference_df = spark.read.option(\"multiline\", \"true\").option(\"encoding\", \"UTF-8\").json(f\"{base_path}{reference_file}\")\n",
    "    reference_df.printSchema()\n",
    "    \n",
    "    print(f\"\\n7. Record Count Analysis:\")\n",
    "    total_estimated_records = 0\n",
    "    for file_name in successful_reads:\n",
    "        count = record_counts[file_name]\n",
    "        print(f\"   {file_name}: {count:,} records\")\n",
    "        total_estimated_records += count\n",
    "    \n",
    "    avg_records = total_estimated_records / len(successful_reads)\n",
    "    estimated_total = avg_records * 45  # All 45 files\n",
    "    \n",
    "    print(f\"\\n   Average records per file: {avg_records:,.0f}\")\n",
    "    print(f\"   Estimated total records (45 files): {estimated_total:,.0f}\")\n",
    "    \n",
    "    print(f\"\\n8. Sample Data from {reference_file}:\")\n",
    "    reference_df.show(3, truncate=False)\n",
    "    \n",
    "    print(f\"\\n=== EXPLORATION COMPLETE ===\")\n",
    "    \n",
    "    if all_schemas_match and len(successful_reads) >= 3:\n",
    "        print(f\"✅ RECOMMENDATION: Use wildcard pattern approach - schemas are consistent\")\n",
    "        print(f\"   Ready for bronze table creation\")\n",
    "    elif len(successful_reads) >= 3:\n",
    "        print(f\"⚠️  RECOMMENDATION: Use union approach with schema handling\")\n",
    "    else:\n",
    "        print(f\"❌ RECOMMENDATION: Investigate file access issues\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ No files could be read successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98bc4e24-0120-4a11-bf89-7f6ef1a47142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"=== DRUG PRODUCT PERMISSION INFO - BRONZE TABLE CREATION ===\")\n",
    "\n",
    "# Configuration\n",
    "base_path = '/Volumes/main/default/medical_data_volume/medical_data_complete_unzipped/medical_data_local/drug-product-permission-info/'\n",
    "wildcard_path = f\"{base_path}chunk_*.json\"\n",
    "bronze_table_name = \"main.default.drug_product_permission_bronze\"\n",
    "\n",
    "print(f\"\\nSource path: {wildcard_path}\")\n",
    "print(f\"Target table: {bronze_table_name}\")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"\\n1. Reading all 45 JSON files using wildcard pattern...\")\n",
    "try:\n",
    "    # Read all files at once using wildcard pattern\n",
    "    df = spark.read \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .option(\"encoding\", \"UTF-8\") \\\n",
    "        .json(wildcard_path)\n",
    "    \n",
    "    print(f\"✅ Successfully read all files\")\n",
    "    \n",
    "    # Quick validation\n",
    "    total_records = df.count()\n",
    "    total_columns = len(df.columns)\n",
    "    \n",
    "    print(f\"   Total records: {total_records:,}\")\n",
    "    print(f\"   Total columns: {total_columns}\")\n",
    "    \n",
    "    print(f\"\\n2. Data Quality Checks:\")\n",
    "    \n",
    "    # Check for null values in key columns\n",
    "    key_columns = [\"ITEM_SEQ\", \"ITEM_NAME\", \"ENTP_NAME\", \"ITEM_PERMIT_DATE\", \"ETC_OTC_CODE\"]\n",
    "    for col_name in key_columns:\n",
    "        null_count = df.filter(col(col_name).isNull() | (col(col_name) == \"\")).count()\n",
    "        print(f\"   {col_name}: {null_count:,} null/empty values\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    distinct_records = df.distinct().count()\n",
    "    duplicate_count = total_records - distinct_records\n",
    "    print(f\"   Duplicate records: {duplicate_count:,}\")\n",
    "    \n",
    "    # Show schema (abbreviated for readability)\n",
    "    print(f\"\\n3. Schema Overview (42 columns):\")\n",
    "    schema_fields = df.dtypes\n",
    "    print(f\"   Key regulatory columns:\")\n",
    "    regulatory_cols = [\"ITEM_SEQ\", \"ITEM_NAME\", \"ENTP_NAME\", \"ITEM_PERMIT_DATE\", \n",
    "                      \"ETC_OTC_CODE\", \"PERMIT_KIND_NAME\", \"ATC_CODE\", \"NARCOTIC_KIND_CODE\"]\n",
    "    for col_name in regulatory_cols:\n",
    "        col_type = dict(schema_fields)[col_name] if col_name in dict(schema_fields) else \"Not found\"\n",
    "        print(f\"     {col_name}: {col_type}\")\n",
    "    \n",
    "    print(f\"\\n4. Creating Bronze Table...\")\n",
    "    \n",
    "    # Write to Delta table\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(bronze_table_name)\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    print(f\"✅ Successfully created bronze table\")\n",
    "    print(f\"   Processing time: {processing_time:.2f} seconds\")\n",
    "    print(f\"   Records per second: {total_records/processing_time:,.0f}\")\n",
    "    \n",
    "    print(f\"\\n5. Bronze Table Verification:\")\n",
    "    \n",
    "    # Verify the table was created\n",
    "    bronze_df = spark.table(bronze_table_name)\n",
    "    verified_count = bronze_df.count()\n",
    "    \n",
    "    print(f\"   Records in bronze table: {verified_count:,}\")\n",
    "    print(f\"   Data integrity: {'✅ Passed' if verified_count == total_records else '❌ Failed'}\")\n",
    "    \n",
    "    print(f\"\\n6. Sample Data from Bronze Table:\")\n",
    "    bronze_df.select(\n",
    "        \"ITEM_NAME\", \n",
    "        \"ENTP_NAME\", \n",
    "        \"ITEM_PERMIT_DATE\",\n",
    "        \"ETC_OTC_CODE\",\n",
    "        \"PERMIT_KIND_NAME\",\n",
    "        \"ATC_CODE\"\n",
    "    ).show(5, truncate=False)\n",
    "    \n",
    "    print(f\"\\n7. Business Intelligence Summary:\")\n",
    "    \n",
    "    # ETC/OTC distribution\n",
    "    print(f\"   Prescription vs OTC Distribution:\")\n",
    "    bronze_df.groupBy(\"ETC_OTC_CODE\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .show(10, truncate=False)\n",
    "    \n",
    "    # Top enterprises by drug count\n",
    "    print(f\"   Top 10 Enterprises by Drug Product Count:\")\n",
    "    bronze_df.groupBy(\"ENTP_NAME\") \\\n",
    "        .agg(countDistinct(\"ITEM_SEQ\").alias(\"unique_products\"),\n",
    "             count(\"*\").alias(\"total_records\")) \\\n",
    "        .orderBy(col(\"unique_products\").desc()) \\\n",
    "        .show(10, truncate=False)\n",
    "    \n",
    "    # Permission types distribution\n",
    "    print(f\"   Permission Types Distribution:\")\n",
    "    bronze_df.groupBy(\"PERMIT_KIND_NAME\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .show(10, truncate=False)\n",
    "    \n",
    "    # ATC code categories (first level)\n",
    "    print(f\"   ATC Code Categories (First Level):\")\n",
    "    bronze_df.filter(col(\"ATC_CODE\").isNotNull() & (col(\"ATC_CODE\") != \"\")) \\\n",
    "        .withColumn(\"ATC_FIRST_LEVEL\", substring(col(\"ATC_CODE\"), 1, 1)) \\\n",
    "        .groupBy(\"ATC_FIRST_LEVEL\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .show(15, truncate=False)\n",
    "    \n",
    "    print(f\"\\n=== INGESTION COMPLETED SUCCESSFULLY ===\")\n",
    "    print(f\"🎉 Bronze table '{bronze_table_name}' created with {verified_count:,} records\")\n",
    "    print(f\"📊 Ready for silver layer transformations and regulatory analysis\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during ingestion: {str(e)}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Provide debugging info\n",
    "    print(f\"\\nDebugging information:\")\n",
    "    print(f\"- Verify all 45 files are accessible\")\n",
    "    print(f\"- Check Spark cluster memory (42 columns × 42K records)\")\n",
    "    print(f\"- Consider batch processing if memory constraints exist\")\n",
    "    \n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze-Drug Permission Info",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
